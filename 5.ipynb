{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn=neighbors.KNeighborsClassifier(n_neighbors=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit([[1],[2],[3],[4],[5],[6]], [0,0,0,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict_proba(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict_proba(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5,  0.5]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict_proba(3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "code_match=re.compile('<pre>(.*?)</pre>',re.MULTILINE|re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "link_match=re.compile('a href=\"http://.*?\".*?>(.*?)</a>',re.MULTILINE|re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features_from_body(s):\n",
    "    link_count_in_code=0\n",
    "    for match_str in code_match.findall(s):\n",
    "        link_count_in_code+=len(link_match.findall(match_str))\n",
    "    return len(link_match.findall(s))-link_count_in_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features_from_body(s):\n",
    "    num_code_lines=0\n",
    "    link_count_in_code=0\n",
    "    code_free_s=s\n",
    "    for match_str in code_match.findall(s):\n",
    "        num_code_lines+=match_str.count('\\n')\n",
    "        code_free_s=code_match.sub('',code_free_s)\n",
    "        link_count_in_code+=len(link_match.findall(match_str))\n",
    "        links=link_match.findall(s)\n",
    "        link_count=len(links)\n",
    "        link_count-=link_count_in_code\n",
    "        html_free_s=re.sub(\" +\",\" \",tag_match.sub('',code_free_s)).replace('\\n','')\n",
    "        link_free_s=html_free_s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=np.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False, False,  True], dtype=bool)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logical_or(x<1,x>3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_ngram_model():\n",
    "    tfidf_ngrams=TfidfVectorizer(ngram_range=(1,3),analyzer='word',binary=False)\n",
    "    clf=MultinomialNB()\n",
    "    pipeline=Pipeline([('vect',tfidf_ngrams),('clf',clf)])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve,auc\n",
    "from sklearn.cross_validation import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(clf_factory,X,Y):\n",
    "    cv=ShuffleSplit(n=len(X),n_iter=10,test_size=0.3,indices=True,random_state=0)\n",
    "    scores=[]\n",
    "    pr_scores=[]\n",
    "    for train,test in cv:\n",
    "        X_train,y_train=X[train],Y[train]\n",
    "        X_test,y_test=X[test],Y[test]\n",
    "        clf=clf_factory()\n",
    "        clf.fit(X_train,y_train)\n",
    "        train_score=clf.score(X_train,y_train)\n",
    "        test_score=clf.score(X_test,y_test)\n",
    "        scores.append(test_score)\n",
    "        proba=clf.predict_proba(X_test)\n",
    "        precision,recall,pr_thresholds=precision_recall_curve(y_test,proba[:,1])\n",
    "        pr_scores.append(auc(recall,precision))\n",
    "        summary=(np.mean(scores),np.std(scores),np.mean(pr_scores),np.std(pr_scores))\n",
    "        print '%.3f\\t%.3f\\t%.3f\\t%.3f'%summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_model(clf_facotry,X,Y):\n",
    "    cv=ShuffleSplit(n=len(X),n_iter=10,test_size=0.3,indices=True,random_state=0)\n",
    "    param_grid=dict(vect_ngram_range=[(1,1),(1,2),(1,3)],vect_min_df=[1,2],vect_stop_words=[None,'english'],\n",
    "                    vect_smooth_idf=[False,True],vect_use_idf=[False,True],vect_sublinear_tf=[False,True],\n",
    "                    clf_alpha=[0,0.01,0.05,0.1,0.5,1],)\n",
    "    grid_search=GridSearchCV(clf_facotry(),param_grid=parma_grid,cv=cv,score_func=f1_score,verbose=10)\n",
    "    grid_search.fit(X,Y)\n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.word_tokenize('This is a good book.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv,collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sent_word_net():\n",
    "    sent_scores=collections.defaultdict(list)\n",
    "    with open(os.path.join(DATA_DIR,'SentiWordNet_3.0.0_20130122.txt'),'r') as csvfile:\n",
    "        reader=csv.reader(csvfile,delimiter='\\t',quotechar='\"')\n",
    "        for line in reader:\n",
    "            if line[0].startswith('#'):\n",
    "                continue\n",
    "            if len(line)==1:\n",
    "                continue\n",
    "            Pos,ID,PosScore,NegScore,SynsetTerms,Gloss=line\n",
    "            if len(POS)==0 or len(ID)==0:\n",
    "                continue\n",
    "            for term in SynsetTerms.split(' '):\n",
    "                term=term.split('#')[0]\n",
    "                term=term.replace('-',' ').replace('_',' ')\n",
    "                key='%s/%s'%(POS,term.split('#')[0])\n",
    "                sent_scores[key].append((float(PosScore),float(NegScore)))\n",
    "    for key,value in sent_scores.iteritems():\n",
    "        sent_scores[key]=np.mean(value,axis=0)\n",
    "    return sent_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_word_net=load_sent_word_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinguisticVectorizer(BaseEstimator):\n",
    "    def get_feature_names(self):\n",
    "        return np.array(['sent_neut','sent_pos','sent_neg','nouns','adjectives','verbs','adverbs',\n",
    "                        'allcaps','exclamation','question','hashtag','mentioning'])\n",
    "    def fit(self,documents,y=none):\n",
    "        return self\n",
    "    def _get_sentiments(self,d):\n",
    "        sent=tuple(d.split())\n",
    "        tagged=nltk.pos_tag(sent)\n",
    "        pos_vals=[]\n",
    "        neg_vals=[]\n",
    "        nouns=0.\n",
    "        adjectives=0.\n",
    "        verbs=0.\n",
    "        adverbs=0.\n",
    "        for w,t in tagged:\n",
    "            p,n=0,0\n",
    "            sent_pos_type=None\n",
    "            if t.startswith('NN'):\n",
    "                sent_pos_type='n'\n",
    "                nouns+=1\n",
    "            elif t.startswith('JJ'):\n",
    "                sent_pos_type='a'\n",
    "                adjectives+=1\n",
    "            elif t.startswith('VB'):\n",
    "                sent_pos_type='v'\n",
    "                verbs+=1\n",
    "            elif t.startswith('RB'):\n",
    "                sent_pos_type='r'\n",
    "                adverbs+=1\n",
    "            if sent_pos_type is not None:\n",
    "                sent_word='%s/%s'%(sent_pos_type,w)\n",
    "                if sent_word in sent_word_net:\n",
    "                    p,n=sent_word_net[sent_word]\n",
    "            pos_vals.append(p)\n",
    "            pos_vals.append(n)\n",
    "        l=len(sent)\n",
    "        avg_pos_val=np.mean(pos_vals)\n",
    "        avg_neg_val=np.mean(neg_vals)\n",
    "        return [1-avg_pos_val-avg_neg_val,avg_pos_val,avg_neg_val,nouns/1,adjectives/1,verbs/1,adverbs/1]\n",
    "    def transform(self,documents):\n",
    "        obj_val,pos_val,neg_val,nouns,adjectives,verbs,adverbs=np.array([self._get_sentiments(d) for d in documents]).T\n",
    "        allcaps=[]\n",
    "        exclamation=[]\n",
    "        question=[]\n",
    "        hashtag=[]\n",
    "        mentioning=[]\n",
    "        for d in documents:\n",
    "            allcaps.append(np.sum([t.isupper() for t in d.split() if len(t)>2]))\n",
    "            exclamation.append(d.count('!'))\n",
    "            question.append(d.count('?'))\n",
    "            hashtag.append(d.cpint('#'))\n",
    "            mentioning.append(d.count('@'))\n",
    "        result=np.array([obj_val,pos_val,neg_val,nouns,adjectives,verbs,adverbs,allcaps,exclamation,question,hashtag,mentioning]).T\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_union_model(params=None):\n",
    "    def preprocessor(tweet):\n",
    "        tweet=tweet.lower()\n",
    "        for k in emo_repl_order:\n",
    "            tweet=tweet.replace(k,emo_repl[k])\n",
    "        for r,repl in re_repl.iteritems():\n",
    "            tweet=re.sub(r,repl,tweet)\n",
    "        return tweet.replace('-',' ').replace('_',' ')\n",
    "    tfidf_ngrams=TfidfVectorizer(preprocessor=preprocessor,analyzer='word')\n",
    "    ling_stats=LinguisticVectorizer()\n",
    "    all_features=FeatureUnion([('ling',ling_stats),('tfidf',tfidf_ngrams)])\n",
    "    clf=MultinomialNB()\n",
    "    pipeline=Pipeline([('all',all_features),('clf',clf)])\n",
    "    if params:\n",
    "        pipeline.set_params(**params)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
